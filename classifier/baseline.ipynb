{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np, pandas as pd, dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quasi-requirements:\n",
    "# !pip install -U scikit-learn, xgboost, pandas, dill, tqdm\n",
    "\n",
    "# # if use postgres\n",
    "# ! pip install psycopg2, psycopg2-binary, sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path_to_postgresql_credentials.txt') as f:\n",
    "    psql = create_engine(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_ann = pd.read_csv('ru_annotation.csv')\n",
    "ru_ann = ru_ann.loc[ru_ann.is_other == 0].copy()\n",
    "\n",
    "uk_ann = pd.read_csv('uk_annotation.csv')\n",
    "uk_ann = uk_ann.loc[uk_ann.is_other == 0].copy()\n",
    "\n",
    "# to use in postgres query - if you will get texts in other way - don't do that\n",
    "ru_ann_ids = ', '.join(ru_ann.html_id.astype(str).values)\n",
    "uk_ann_ids = ', '.join(uk_ann.html_id.astype(str).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "Build TF-IDF vectorizer on random sample of 100k texts for use in training.<br>\n",
    "We do it because there are not so much annotated data, so it is reasonable to use a lot of collected but unlabeled data to train vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_ids = ', '.join(pd.read_sql(\"select html_id from htmls where is_other < 0.68 and lang = 'ru';\", psql\n",
    "                              ).sample(100000).html_id.astype(str).values\n",
    "                  )\n",
    "ru_texts = pd.read_sql(f'''select html_id, tokenized from htmls where html_id in ({ru_ids})''',\n",
    "                       psql, chunksize=10000)\n",
    "del ru_ids\n",
    "\n",
    "uk_ids = ', '.join(pd.read_sql(\"select html_id from htmls where is_other < 0.68 and lang = 'uk';\", psql\n",
    "                              ).sample(100000).html_id.astype(str).values\n",
    "                  )\n",
    "uk_texts = pd.read_sql(f'''select html_id, tokenized from htmls where html_id in ({uk_ids})''',\n",
    "                       psql, chunksize=10000)\n",
    "del uk_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=70000, max_df=0.97)\n",
    "ru_tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=70000, max_df=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen(dfs):\n",
    "    for df in tqdm(dfs):\n",
    "        for text in df.tokenized.values:\n",
    "            if text: yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_tfidf.fit(text_gen(uk_texts))\n",
    "ru_tfidf.fit(text_gen(ru_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save it\n",
    "# with open('uk_tfidf.pkl', 'wb') as ukf, open('ru_tfidf.pkl', 'wb') as ruf:\n",
    "#     pickle.dump(uk_tfidf, ukf)\n",
    "#     pickle.dump(ru_tfidf, ruf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uk_tfidf.pkl', 'rb') as ukf, open('ru_tfidf.pkl', 'rb') as ruf:\n",
    "    uk_tfidf = pickle.load(ukf)\n",
    "    ru_tfidf = pickle.load(ruf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tokenized texts from DB. There is a script to preprocess and tokenize in `data_collection` folder of this repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_ann = uk_ann.merge(\n",
    "    pd.read_sql(f'''\n",
    "    select html_id, lower(tokenized) as tokenized from htmls where html_id in ({uk_ann_ids})\n",
    "    ''', psql), on='html_id', how='left'\n",
    ")\n",
    "\n",
    "ru_ann = ru_ann.merge(\n",
    "    pd.read_sql(f'''\n",
    "    select html_id, lower(tokenized) as tokenized from htmls where html_id in ({ru_ann_ids})\n",
    "    ''', psql), on='html_id', how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add test set to val - just to increase size of validation set\n",
    "\n",
    "ru_test = pd.read_json('ru_test_set.jl', lines=True\n",
    "           ).reindex(['html_id', 'emo_check', 'arg_check', 'is_other', 'trn_val'], axis=1)\n",
    "uk_test = pd.read_json('uk_test_set.jl', lines=True\n",
    "           ).reindex(['html_id', 'emo_check', 'arg_check', 'is_other', 'trn_val'], axis=1)\n",
    "\n",
    "ru_test.is_other = 0\n",
    "uk_test.is_other = 0\n",
    "\n",
    "ru_test.trn_val = 'val'\n",
    "uk_test.trn_val = 'val'\n",
    "\n",
    "uk_test = uk_test.dropna()\n",
    "\n",
    "ru_test_ids = ', '.join(ru_test.html_id.astype(str).values)\n",
    "uk_test_ids = ', '.join(uk_test.html_id.astype(str).values)\n",
    "\n",
    "uk_test = uk_test.merge(\n",
    "    pd.read_sql(f'''\n",
    "    select html_id, lower(tokenized) as tokenized\n",
    "    from htmls\n",
    "    where html_id in ({uk_test_ids})\n",
    "    ''', psql), on='html_id', how='left'\n",
    ").dropna()\n",
    "\n",
    "ru_test = ru_test.merge(\n",
    "    pd.read_sql(f'''\n",
    "    select html_id, lower(tokenized) as tokenized\n",
    "    from htmls where html_id in ({ru_test_ids})\n",
    "    ''', psql), on='html_id', how='left'\n",
    ").dropna()\n",
    "\n",
    "ru_test.columns = ['html_id', 'is_emo', 'is_arg', 'is_other', 'trn_val', 'tokenized']\n",
    "uk_test.columns = ['html_id', 'is_emo', 'is_arg', 'is_other', 'trn_val', 'tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_trn = uk_ann.loc[(uk_ann.trn_val == 'trn') & uk_ann.tokenized.notnull()].copy().sample(frac=1)\n",
    "ru_trn = ru_ann.loc[(ru_ann.trn_val == 'trn') & ru_ann.tokenized.notnull()].copy().sample(frac=1)\n",
    "\n",
    "\n",
    "uk_val = pd.concat([uk_test,\n",
    "                    uk_ann.loc[(uk_ann.trn_val == 'val') & uk_ann.tokenized.notnull()].copy()]\n",
    "          ).drop_duplicates('html_id'\n",
    "          ).sample(frac=1)\n",
    "uk_val = uk_ann.loc[(uk_ann.trn_val == 'val') & uk_ann.tokenized.notnull()].copy().sample(frac=1)\n",
    "\n",
    "ru_val = pd.concat([ru_test,\n",
    "                    ru_ann.loc[(ru_ann.trn_val == 'val') & ru_ann.tokenized.notnull()].copy()]\n",
    "          ).drop_duplicates('html_id'\n",
    "          ).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classifier itself - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_ru_emo = xgb.sklearn.XGBClassifier()\n",
    "cls_ru_arg = xgb.sklearn.XGBClassifier()\n",
    "cls_uk_emo = xgb.sklearn.XGBClassifier()\n",
    "cls_uk_arg = xgb.sklearn.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_trn_vectorized = uk_tfidf.transform(uk_trn.tokenized)\n",
    "cls_uk_emo.fit(uk_trn_vectorized, uk_trn.is_emo)\n",
    "cls_uk_arg.fit(uk_trn_vectorized, uk_trn.is_arg)\n",
    "\n",
    "ru_trn_vectorized = ru_tfidf.transform(ru_trn.tokenized)\n",
    "cls_ru_emo.fit(ru_trn_vectorized, ru_trn.is_emo)\n",
    "cls_ru_arg.fit(ru_trn_vectorized, ru_trn.is_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ru_emo = make_pipeline(ru_tfidf, cls_ru_emo)\n",
    "pipe_ru_arg = make_pipeline(ru_tfidf, cls_ru_arg)\n",
    "pipe_uk_emo = make_pipeline(uk_tfidf, cls_uk_emo)\n",
    "pipe_uk_arg = make_pipeline(uk_tfidf, cls_uk_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_val['pred_emo'] = pipe_ru_emo.predict_proba(ru_val.tokenized)[:, 1]\n",
    "uk_val['pred_emo'] = pipe_uk_emo.predict_proba(uk_val.tokenized)[:, 1]\n",
    "ru_val['pred_arg'] = pipe_ru_arg.predict_proba(ru_val.tokenized)[:, 1]\n",
    "uk_val['pred_arg'] = pipe_uk_arg.predict_proba(uk_val.tokenized)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(df, which, thr):\n",
    "    '''\n",
    "    A simple function to print metrics.\n",
    "    df - data frame with val data\n",
    "    which - prefix \"emo\" or \"arg\"\n",
    "    thr - threshold from where we classify value as positive\n",
    "    '''\n",
    "    kwargs = dict(y_true=df[f'is_{which}'], y_pred=df[f'pred_{which}'] > thr, labels=[True, False])\n",
    "    print(classification_report(**kwargs))\n",
    "    cm = confusion_matrix(**kwargs)\n",
    "    print(cm / cm.sum(1)[:, None])\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In baseline we select threshold so that there will be at least some classified true positives and false positive value is less than 10% and there are more true positives than false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ru emo\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.47      0.42      0.45       280\n",
      "       False       0.89      0.91      0.90      1433\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1713\n",
      "   macro avg       0.68      0.67      0.67      1713\n",
      "weighted avg       0.82      0.83      0.82      1713\n",
      "\n",
      "[[0.42142857 0.57857143]\n",
      " [0.09141661 0.90858339]]\n",
      "[[ 118  162]\n",
      " [ 131 1302]]\n",
      "\n",
      "Uk emo\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.43      0.21      0.28        48\n",
      "       False       0.79      0.92      0.85       153\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       201\n",
      "   macro avg       0.61      0.56      0.56       201\n",
      "weighted avg       0.70      0.75      0.71       201\n",
      "\n",
      "[[0.20833333 0.79166667]\n",
      " [0.08496732 0.91503268]]\n",
      "[[ 10  38]\n",
      " [ 13 140]]\n",
      "\n",
      "Ru arg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.50      0.22      0.31       342\n",
      "       False       0.83      0.94      0.88      1371\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1713\n",
      "   macro avg       0.66      0.58      0.60      1713\n",
      "weighted avg       0.76      0.80      0.77      1713\n",
      "\n",
      "[[0.22222222 0.77777778]\n",
      " [0.05616338 0.94383662]]\n",
      "[[  76  266]\n",
      " [  77 1294]]\n",
      "\n",
      "Uk arg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.31      0.15      0.20        27\n",
      "       False       0.88      0.95      0.91       174\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       201\n",
      "   macro avg       0.59      0.55      0.56       201\n",
      "weighted avg       0.80      0.84      0.82       201\n",
      "\n",
      "[[0.14814815 0.85185185]\n",
      " [0.05172414 0.94827586]]\n",
      "[[  4  23]\n",
      " [  9 165]]\n"
     ]
    }
   ],
   "source": [
    "print('Ru emo')\n",
    "print_metrics(ru_val, 'emo', 0.5)\n",
    "\n",
    "print('\\nUk emo')\n",
    "print_metrics(uk_val, 'emo', 0.3)\n",
    "\n",
    "print('\\nRu arg')\n",
    "print_metrics(ru_val, 'arg', 0.1)\n",
    "\n",
    "print('\\nUk arg')\n",
    "print_metrics(uk_val, 'arg', 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
