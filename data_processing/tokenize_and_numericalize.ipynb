{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, _pickle as pickle, re, json\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine\n",
    "from time import sleep, time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "from bz2 import BZ2File\n",
    "\n",
    "# with open('../psql_engine.txt') as f:\n",
    "#     psql = create_engine(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize\n",
    "\n",
    "Turns readability htmls into tokenized text and saves it to database.<br>\n",
    "Uses simple pretrained logistic regression classifier to remove unrelated items: datetime paragraphs, \"read also\" paragraphs etc.<br>\n",
    "Tokenizes Russian text with `nltk` and Ukrainian - using `tokenize_uk`.<br>\n",
    "Select texts not longer than 8000 characters. Longer texts are analytics or articles, they are written differently and are not considered in this research "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import *\n",
    "from tokenize_uk.tokenize_uk import tokenize_sents, tokenize_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postgresql query to filter incorectly loaded articles.<br>No need to use - sample contains only loaded ones.<br>There are around 5% of incorectly loaded texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "htmls = pd.read_json('../htmls_sample.jl.bz2', lines=True, chunksize=1000)\n",
    "\n",
    "# bad = ['Руководство сайта не несет ответственности за достоверность материалов, присланных нашими читателями. Администрация сайта, публикуя статьи наших читателей, предупреждает, что их мнения могут не совпадать',\n",
    "#        'Усі права захищені. Матеріали із сайта',\n",
    "#        'Все права на материалы, опубликованные на данном ресурсе, принадлежат',\n",
    "#        'не подлежат дальнейшему воспроизведению и/или распространению в какой-либо форме',\n",
    "#        'При цитуванні і використанні будь-яких матеріалів в Інтернеті',\n",
    "#        'Регистрация пользователя в сервисе РИА Клуб на сайте Ria.Ru и авторизация',\n",
    "#        '© Автономная некоммерческая организация «ТВ-Новости», 2005—2017 гг. Все права защищены', \n",
    "#        'Лидер © 2001-2017 UA-Reporter.com Первое информационное интернет-издание Закарпатской области', \n",
    "#        'Ваш регион:   Основной сайт Москва Северо-Запад Урал Сибирь',\n",
    "#        '18+ Настоящий ресурс может содержать материалы 18+ При цитировании информации гиперссылка на ИА',\n",
    "#        'Введіть слово, щоб почати',\n",
    "#        'Все об украинской политике, олигархи, которые руководят Украиной, Янукович, украинский парламент, политические новости, выборы, интервью с известными политиками, мнения политтехнологов, новостис регионов, комментарии читателей, ТОП 100 влиятельных украинцев,коррупция в украинской политике, криминал украинской политики,новости Верховного Совета Украины',\n",
    "#        'Все права защищены Все права на материалы, опубликованные на данном ресурсе, принадлежат ООО',\n",
    "#        'Любое использование материалов c сайта или программ телеканала 112 Украина разрешается при согласовании с редакцией',\n",
    "#        'Материалы, содержащие отметку Пресс-релиз, могут быть опубликованы на правах рекламы. Материалы с пометкой',\n",
    "#        'News24UA - новости Украины, новости политики, самые свежие новости экономики, общества и криминала Новости сегодня в Украине Верховная Рада Украины, новые законопроекты, комментарии украинских политиков и парламентариев',\n",
    "#        'новости Украины, новости украины сегодня, последние новости украины, новости часа, новости дня, новости онлайн, последние новости в украине',\n",
    "#        'Похоже, что вы используете блокировщик рекламы :\\(Чтобы пользоваться всеми функциями сайта',\n",
    "#        '- \"INSIDER LIFE NEWS\" - insiderlifenews@gmail.com 2014 .',\n",
    "#        '2014-2017 . . \\r . \\r \" \\(\\) \" \\r . 34 . \" \" -',\n",
    "#        'Copyright © 1999-2018, технология и дизайн принадлежат ООО «Правда.Ру»',\n",
    "#        'выдано Федеральной службой по надзору в сфере связи, информационных технологий и массовых коммуникаций',\n",
    "#        'Перепечатка, копирование или воспроизведение информации, опубликованной на сайте',\n",
    "#        'Введите слово, чтобы начать',\n",
    "#        'Надежные VPS/VDS, выделенные серверы и хостинг',\n",
    "#        'влажность: давление: ветер:',\n",
    "#        'Усі права захищено. Думка автора статті не відображає думку редакції',\n",
    "#        'Использование материалов и новостей Сегодня разрешается при условии ссылки на Сегодня.ua',\n",
    "#        'PolitCentr.ru 2013-2017',\n",
    "#        'Читайте виртуальные журналы RT на русском в Flipboard',\n",
    "#        'Наша цель – актуальное освещение событий касающихся политической ситуации в Украине и вокруг неё, войны на территории Украины, ситуации сложившейся в Крыму',\n",
    "#        'Войдите через социальные сети: или авторизуйтесь:',\n",
    "#        'Мнение редакции может не совпадать с точкой зрения авторов публикаций']\n",
    "\n",
    "# bad_search_q = '\\nor '.join(f'''ra_summary ~~* '%%{cr.replace(\"'\", \"''\").replace('%', '%%')}%%' '''\n",
    "#                             for cr in bad)\n",
    "\n",
    "# q = f'''\n",
    "# SELECT html_id, ra_title, ra_summary, real_url, link, lang FROM htmls\n",
    "# WHERE tokenized isnull\n",
    "#       and not (LENGTH(REGEXP_replace(ra_summary, '[^А-Яа-яІіЇїЄє]', '')) < 20 OR\n",
    "#               ra_summary = '<html><body/></html>'\n",
    "#               or ra_summary ~* '[‡ЂЏЎЋѓЃЊµ]'\n",
    "#               or ra_summary isnull\n",
    "#               or {bad_search_q}\n",
    "#       )\n",
    "#       and LENGTH(REGEXP_replace(ra_summary, '[^А-Яа-яІіЇїЄє]', '')) < 7000;\n",
    "# '''\n",
    "\n",
    "# htmls = pd.read_sql(q, psql, chunksize=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load paragraph classifier - scikit-learn logistic regression, and DictVertorizer for feature transformation.<br>\n",
    "Features:\n",
    "* html tag name,\n",
    "* its classes,\n",
    "* and id, \n",
    "* Length of inner text,\n",
    "* bag of words of element contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dvect_tech_tags.pkl', 'rb') as f:\n",
    "    dv = pickle.load(f)\n",
    "    \n",
    "with open('classify_tech_tags.pkl', 'rb') as f:\n",
    "    cls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats(tag):\n",
    "    '''\n",
    "    returns feature dict for paragraph classifier.\n",
    "    features: htmls tag name, its classes, and id. Length of inner text and bag of words of element contents.\n",
    "    '''\n",
    "    name = tag.name\n",
    "    \n",
    "    text = re.sub('[^A-zА-яІіЇїЄєҐґ0-9 ]', ' ', tag.get_text(' ')).split()\n",
    "    text = [re.sub('[0-9]', '5', str(i)) for i in text]\n",
    "\n",
    "    class_full = tag.get('class')\n",
    "    if class_full is not None:\n",
    "        classes = list(chain(*[re.split('-+|_+', cl) for cl in class_full]))\n",
    "    else:\n",
    "        classes = []\n",
    "\n",
    "    id_full = tag.get('id')\n",
    "    id_list = re.split('-+|_+', id_full.lower()) if id_full else []\n",
    "    \n",
    "    words = [f'word_{w.lower()}' for w in text]\n",
    "    cls_feats = [f'class_{w}' for w in classes]\n",
    "\n",
    "    id_feats = [f'id_{w}' for w in id_list]\n",
    "\n",
    "    feats = {**Counter(cls_feats), **Counter(id_feats), **Counter(words)}\n",
    "    feats['len'] = len(words)\n",
    "\n",
    "    feats[f'tagname_{name}'] = 1\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_html(row):\n",
    "    soup = BeautifulSoup(re.sub('<br/?>', '</p><p>', row.ra_summary), 'lxml')\n",
    "    [t.extract() for t in soup.select('div#more-items-infinite, div.fb-post, p.go_out, div.twitter-tweet')]\n",
    "    \n",
    "    [t.extract() for t in soup.find_all('h1', text='Новини по темі')]\n",
    "    [t.extract() for t in soup.find_all('p', text=re.compile(\n",
    "        'в даний момент ви читаєте новину|в даный момент вы читаете новость .* на [(eizvestia)|(enovosti)]\\.com',\n",
    "        flags=re.I\n",
    "    ))]\n",
    "    url = row.real_url if row.real_url else row.link\n",
    "    url = url if url else ''\n",
    "    if 'ria.ru' in url:\n",
    "        ria_caption = soup.select('div[itemprop=\"articleBody\"] strong')\n",
    "        if len(ria_caption) != 0: ria_caption[0].extract()\n",
    "\n",
    "    hs = soup.select('h1, h2, h3, h4, h5')\n",
    "    paragraphs = [t for t in soup.find_all() \n",
    "                  if t.name in ['h1','h2','h3','h4','h5','h6','p', 'div', 'li']\n",
    "                     and not t.p\n",
    "                     and not t.div]\n",
    "    \n",
    "    intersection = list(filter(lambda t: t in paragraphs[:5], hs))\n",
    "    if len(intersection) != 0:\n",
    "        del paragraphs[paragraphs.index(intersection[0])]\n",
    "        \n",
    "    if len(paragraphs) == 0: return\n",
    "    \n",
    "    X = dv.transform(list(map(get_feats, paragraphs)))\n",
    "    preds = cls.predict_proba(X)[:, 0]\n",
    "    paragraps = [p for pred, p in zip(preds.tolist(), paragraphs) if pred > 0.18]    \n",
    "    paragraphs = [re.sub('\\s+', ' ', p.get_text(' ').replace('\\xa0', ' ')).strip() for p in paragraphs]\n",
    "    paragraphs = list(filter(lambda p: len(re.sub('[^A-zА-яІіЇїЄєҐґ]', '', p)) > 2, paragraphs))\n",
    "    \n",
    "    if row.lang == 'uk':\n",
    "        tokenized = '\\n\\n'.join('\\n'.join([' '.join(tokenize_words(s))\n",
    "                                           for s in tokenize_sents(p)])\n",
    "                                for p in paragraphs)\n",
    "    elif row.lang == 'ru':\n",
    "        tokenized = '\\n\\n'.join('\\n'.join([' '.join(word_tokenize(s))\n",
    "                                           for s in sent_tokenize(p)])\n",
    "                                for p in paragraphs)\n",
    "    return re.sub(\"``|''\", '\"', tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BZ2File('tokenized_htmls.jl.bz2', 'w') as f:\n",
    "    for df in tqdm(htmls):\n",
    "        df['tokenized'] = df.apply(tokenize_html, axis=1)\n",
    "        df = df.loc[pd.notnull(df.tokenized)\n",
    "              ].reindex(['html_id', 'tokenized', 'lang'], axis=1\n",
    "              ).copy()\n",
    "        \n",
    "        f.write(\n",
    "            (df.to_json(orient='records', lines=True, force_ascii=False) + '\\n'\n",
    "            ).encode('utf-8')\n",
    "        )\n",
    "#         # in case of postgres\n",
    "#     df.to_json('tokenized_htmls.jl.bz2', compression='bz2', )\n",
    "#     vals = ', '.join([f'''({html_id}, '{tok.replace(\"'\", \"''\").replace('%', '%%')}')'''\n",
    "#                       for html_id, tok in df.loc[pd.notnull(df.tokenized)].reindex(['html_id', 'tokenized'], axis=1).values])\n",
    "#     psql.execute(f'''\n",
    "#                   update htmls as t set\n",
    "#                   tokenized = c.tok\n",
    "#                   from (values\n",
    "#                       {vals}\n",
    "#                   ) as c(html_id, tok) \n",
    "#                   where c.html_id = t.html_id;\n",
    "#                   ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## str2id\n",
    "\n",
    "Transforms space delimited tokenized text into an array of word ids according to dictionary. Uses separate dictionaries for Ukrainian and Russian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = pd.read_json('tokenized_htmls.jl.bz2', lines=True, chunksize=1000)\n",
    "# tokenized = pd.read_sql('''\n",
    "#                         select html_id, tokenized, lang from htmls\n",
    "#                         where tokenized notnull\n",
    "#                           and word_ids isnull;\n",
    "#                         ''', psql, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos' #beginning of string tag\n",
    "FLD = 'xbod' #beginning of doc tag\n",
    "\n",
    "with open('itos_ru.pkl', 'rb') as f:\n",
    "    itos_ru = pickle.load(f)\n",
    "    #reverse  - return id for every word, id \"0\" for out of dictionary tokens \n",
    "    stoi_ru = defaultdict(lambda: 0, {v: k for k, v in enumerate(itos_ru)})\n",
    "with open('itos_uk.pkl', 'rb') as f:\n",
    "    itos_uk = pickle.load(f)\n",
    "    stoi_uk = defaultdict(lambda: 0, {v: k for k, v in enumerate(itos_uk)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`itos` - dictionary for language composed of all loaded news texts. Up to 60000 tokens that occur more than 15 times in all news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tok(text):\n",
    "    return' \\n '.join([p.replace('\\n', f' {BOS} ')\n",
    "                       for p in text.strip().split('\\n\\n')]\n",
    "                     ).split(' ')\n",
    "\n",
    "def tok2id(row):\n",
    "    stoi = stoi_ru if row.lang == 'ru' else stoi_uk\n",
    "    return [stoi_ru[token] for token in split_tok(row.tokenized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BZ2File('word_ids_htmls.jl.bz2', 'w') as f:\n",
    "    for df in tqdm(tokenized):\n",
    "        df.tokenized = f'\\n{BOS} {FLD} 1 ' + df.tokenized\n",
    "        # \"1\" is a mistake, but since it occurs in every article, and at the beginning,\n",
    "        # I believe it doesn't influence the result - LSTM will forget it by the end of text and will not find it meaningful\n",
    "        df['word_ids'] = df.apply(tok2id, axis=1)\n",
    "        df = df.loc[pd.notnull(df.word_ids)\n",
    "              ].reindex(['html_id', 'word_ids'], axis=1\n",
    "              ).copy()\n",
    "        \n",
    "        f.write(\n",
    "            (df.to_json(orient='records', lines=True, force_ascii=False) + '\\n'\n",
    "            ).encode('utf-8')\n",
    "        )\n",
    "#     # for postgres\n",
    "#     vals = ', '.join([f'''({html_id}, ARRAY{str2id})'''\n",
    "#                       for html_id, str2id in df.reindex(['html_id', 'word_ids'], axis=1).values])\n",
    "#     psql.execute(f'''\n",
    "#                   update htmls as t set\n",
    "#                   word_ids = c.word_ids\n",
    "#                   from (values\n",
    "#                       {vals}\n",
    "#                   ) as c(html_id, word_ids) \n",
    "#                   where c.html_id = t.html_id;\n",
    "#                   ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
